{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research\n",
    "\n",
    "After reading a few papers and discovering actual applications, a linkedIn group for question generation and many quora questions and answers, it seems that question generation is not unheard of.\n",
    "\n",
    "I learned some tricks, but I also thought of a method for generating multiple answer questions, that I didn't find anywhere else.\n",
    "\n",
    "### Papers\n",
    "Difficulty Controllable Question Generation for Reading Comprehension - https://arxiv.org/abs/1807.03586  \n",
    "Identifying Where to Focus in Reading Comprehension for Neural Question Generation - https://aclanthology.coli.uni-saarland.de/papers/D17-1219/d17-1219  \n",
    "SQuAD: 100,000+ Questions for Machine Comprehension of Text - https://arxiv.org/abs/1606.05250  \n",
    "Know What You Don't Know: Unanswerable Questions for SQuAD - https://arxiv.org/abs/1806.03822  \n",
    "\n",
    "### Applications\n",
    "Questo AI - https://questo.ai/index.html - In developement  \n",
    "Quillionz - https://blog.quillionz.com/ - Making quiz tests for teachers. Hand curating the generated questions before providing them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data\n",
    "I chose the Stanford Question Answering Dataset (SQaAD) dataset. There are currently two versions of it.  \n",
    "\n",
    "The first version contains 100k questions generated by mechanical turks that were really well paid. The questions are on 536 various wikipedia articles.\n",
    "\n",
    "The second version contains 50k more questions, but they are specifically questions the answer for which cannot be found in the text. This has been done so that the neural nets trying to answer the questions learn when there is insufficient information and to avoid guessing.  \n",
    "\n",
    "Obviously I cannot use the additional 50k questions for my purposes, so I used SQaUD v1.\n",
    "\n",
    "I just found another dataset which could be used in addition to SQuAD - https://www.kaggle.com/rtatman/questionanswer-dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea\n",
    "Instead of trying to generate a question directly from text, with some impossible to train model (which seems to be the idea of most people) I propose a more step by step solution.\n",
    "\n",
    "* Identifying phrases that are worth making a question for and taking the sentence the phrase is contained in.\n",
    "* Transforming that sentence to 'cloze'* type question with the phrase as the right answer.\n",
    "* Generating wrong answers for multiple answers type questions.\n",
    "* Transforming the 'cloze' type question to a question looking... question.\n",
    "\n",
    "\\*Cloze questions are those things: *Today, I went to the ________ and bought some milk and eggs*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying phrases that are worth making a question for\n",
    "\n",
    "For the neural network I guessed that a seq2seq aproach would work. It's commonly used for translating languages, but I could also train it to translate from text to phrases worth asking a question for.\n",
    "\n",
    "#### Preparing data\n",
    "The dataset exploration with some data engineering is done in the 'prepare data' notebook.\n",
    "\n",
    "#### Training the model\n",
    "I trained the model for about for about 1.5h.\n",
    "\n",
    "#### Testing\n",
    "Having in mind my limited time and resources I didn't expect much from my model. And rightfully so. Some of the generated phrases weren't even in the text.\n",
    "\n",
    "#### Results\n",
    "\n",
    "##### Text\n",
    "the exact date of creation of the kiev metropolis is uncertain, as well as who was the first leader of the church. predominantly it is considered that the first head was michael i of kiev, however some sources also claim **leontiy** who is often placed after michael or anastas chersonesos, became the first bishop of the church of the tithes. the first metropolitan to be confirmed by historical sources is theopemp, who was appointed by patriarch alexius of constantinople in ####. before #### there were five dioceses: kiev, chernihiv, bilhorod, volodymyr, novgorod, and soon thereafter yuriy-upon-ros. the kiev metropolitan sent his own delegation to the council of bari in ####.\n",
    "\n",
    "##### Answer\n",
    "**leonity**\n",
    "\n",
    "##### Text\n",
    "during the period in which the populares party controlled the city, they flouted convention by re-electing marius consul several times without observing the customary ten-year interval between offices. they also transgressed the established oligarchy by advancing unelected individuals to magisterial office, and by substituting magisterial edicts for popular legislation. sulla soon made peace with mithridates. in ## bc, he returned to rome, overcame all resistance, and recaptured the city. sulla and his supporters then slaughtered most of marius' supporters. sulla, having observed the violent results of radical popular reforms, was naturally conservative. as such, he sought to strengthen the aristocracy, and by extension the senate. sulla made himself dictator, passed a series of constitutional reforms, resigned the dictatorship, and served one last term as consul. he died in ## bc.\n",
    "\n",
    "##### Answer\n",
    "\\# bc\n",
    "\n",
    "##### Text\n",
    "**oxygen** is a chemical element with symbol O and atomic number 8. It is a member of the chalcogen group on the periodic table and is a highly reactive nonmetal and oxidizing agent that readily forms compounds (notably oxides) with most elements.\n",
    "\n",
    "##### Answer\n",
    "oxygen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullText = 'Oxygen is a chemical element with symbol O and atomic number 8. It is a member of the chalcogen group on the periodic table and is a highly reactive nonmetal and oxidizing agent that readily forms compounds (notably oxides) with most elements. '\n",
    "phrase = 'Oxygen'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming sentences to cloze questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____ is a chemical element with symbol O and atomic number 8\n"
     ]
    }
   ],
   "source": [
    "sentences = fullText.split('.')\n",
    "sentence = []\n",
    "\n",
    "for sentenceIndex in range (len(sentences)):\n",
    "    if phrase in sentences[sentenceIndex]:\n",
    "        sentence = sentences[sentenceIndex]\n",
    "        \n",
    "print(sentence.replace(phrase, '_____'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating wrong answers for multiple answers type questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Load embeddings\n",
    "glove_file = datapath('D:\\ML\\QG\\QG\\data\\embeddings\\glove.6B.300d.txt')\n",
    "tmp_file = get_tmpfile(\"D:\\ML\\QG\\QG\\data\\embeddings\\word2vec-glove.6B.300d.txt\")\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hydrogen', 5), ('nitrogen', 4), ('carbon', 3), ('helium', 2), ('liquid', 2), ('atom', 2), ('nutrient', 1), ('ammonia', 1), ('vapor', 1), ('sulfur', 1), ('breath', 0), ('hemoglobin', 0), ('hypoxia', 0)]\n",
      "['hydrogen', 'nitrogen', 'helium', 'nutrient', 'breath', 'ammonia', 'carbon', 'liquid', 'hemoglobin', 'vapor', 'atom', 'hypoxia', 'sulfur']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#\n",
    "answer = phrase\n",
    "\n",
    "#Stemming\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sentence = stemmer.stem(sentence)\n",
    "answer = stemmer.stem(answer)\n",
    "\n",
    "#Removing stopwords, answer and punctuation from sentence\n",
    "word_list = re.sub(r'[^\\w\\s]','',sentence)\n",
    "word_list = word_list.replace(answer, '').split()\n",
    "\n",
    "filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "\n",
    "#Getting what part of speech the answer is\n",
    "answerPartOfSpeech = wn.synsets(answer) != [] and wn.synsets(answer)[0].pos()\n",
    "\n",
    "##Extracting closest embeddings for the answer\n",
    "topEmbeddings = model.most_similar(positive=[answer], topn=30)\n",
    "\n",
    "embeddings = []\n",
    "for embeddingIndex in range(len(topEmbeddings)):\n",
    "    #Having a threshold. Word embedding shouldn't be further than 0.45\n",
    "    if topEmbeddings[embeddingIndex][1] > 0.45: \n",
    "        word = stemmer.stem(topEmbeddings[embeddingIndex][0])\n",
    "        #Removing words that are not of the same part of speech\n",
    "        if answerPartOfSpeech and wn.synsets(word) != [] and wn.synsets(word)[0].pos() == answerPartOfSpeech:\n",
    "            #Since we are stemming the embeddings, it's possible for a stem to appear more than once\n",
    "            if word not in embeddings:\n",
    "                embeddings.append(word)\n",
    "\n",
    "                \n",
    "#List of occurences for each stemmed word of the original answer in the embeddings for every other word in the sentence\n",
    "embeddingsOccurences =  [0] * len(embeddings)\n",
    "\n",
    "for sentenceWordIndex in range(len(filtered_words)):\n",
    "    senteceWordEmbeddings = model.most_similar(positive=[answer, filtered_words[sentenceWordIndex]], topn=30)\n",
    "    stemmedEmbeddings = []\n",
    "    for embeddingIndex in range(len(senteceWordEmbeddings)):\n",
    "        #Having a threshold. Word embedding shouldn't be further than 0.45\n",
    "        if senteceWordEmbeddings[embeddingIndex][1] > 0.45: \n",
    "            word = stemmer.stem(senteceWordEmbeddings[embeddingIndex][0])\n",
    "            #Since we are stemming the embeddings, it's possible for a stem to appear more than once\n",
    "            if word not in stemmedEmbeddings:\n",
    "                stemmedEmbeddings.append(word)\n",
    "                \n",
    "    for stemmedEmbeddingIndex in range(len(stemmedEmbeddings)):\n",
    "        #Checking if the embedding is also contained in the embedding of the answer\n",
    "        if stemmedEmbeddings[stemmedEmbeddingIndex] in embeddings:\n",
    "            embeddingIndex = embeddings.index(stemmedEmbeddings[stemmedEmbeddingIndex])\n",
    "            embeddingsOccurences[embeddingIndex]+=1\n",
    "            \n",
    "combined = list(zip(embeddings, embeddingsOccurences))\n",
    "bestEmbeddings = sorted(combined, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 3 closest incorrect answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hydrogen\n",
      "nitrogen\n",
      "carbon\n"
     ]
    }
   ],
   "source": [
    "print(bestEmbeddings[0][0])\n",
    "print(bestEmbeddings[1][0])\n",
    "print(bestEmbeddings[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions for which cannot be found sufficient number of good incorrect answers could be turned into true/false questions. \n",
    "The sentences just must be turned into negatives sometimes, as not to be so obvious that every question's answer is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the 'cloze' type question to a question looking... question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it would be easier to train another seq2seq neural network that takes regular sentences and transforms them into questions.  \n",
    "The data could be relatively easy to generate from the SQaUD dataset and they may be other datasets ready for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
